{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Digit Classification from MNIST Dataset.\n",
        "\n",
        "Creating a feed forward neural network for digit classification from the famous MNIST dataset. Also adding GPU support."
      ],
      "metadata": {
        "id": "P_XZh_6R74MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j5DkWXK38H46"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters\n",
        "input_size = 784  # mnist has 28x28 images, which after flattening becomes 784\n",
        "hidden_size = 1000 # number of nodes in hidden layer\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32 # Yann LeCun said mini batch size of more than 32 shouldn't be used.\n",
        "LR = 0.001"
      ],
      "metadata": {
        "id": "lus6MHKg8YR9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                           download=True, transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                           download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "UqG-p2Jf9EM4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at shapes of samples and labels in a single batch.\n",
        "examples = iter(train_loader)\n",
        "samples, labels = next(examples)\n",
        "print(samples.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVWxBcpN97Td",
        "outputId": "3ebeb771-1fca-4f25-8b1f-5ce549c83c23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the shape that our samples has a shape of 32x1x28x28, 32 being the batch size. We also see that our labels are in a 1-D array.\n",
        "\n",
        "We may need to first flatten our samples into an array of 32x784."
      ],
      "metadata": {
        "id": "7LIj06by-Pqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the first 6 images in batch 1.\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)  # creating a grid with 2 rows and 3 cols\n",
        "  plt.imshow(samples[i][0], cmap='gray')  # plotting images with color map = gray\n",
        "  plt.show;  # display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "7OaqHadq-MvW",
        "outputId": "d7d90560-d6cf-4a27-d221-324e8457643f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer_stack(x)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, NUM_CLASSES).to(device)"
      ],
      "metadata": {
        "id": "s_kg7tir_Umy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're not using softmax here at the output layer. This is because our loss function will be `nn.CrossEntropyLoss()`, which expects the predictions as logits. So applying softmax manually would be doing it twice, which makes training slow."
      ],
      "metadata": {
        "id": "NPI9EjwHAlke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LR)"
      ],
      "metadata": {
        "id": "HVGbqIykBtpE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Loop\n",
        "total_batches = len(train_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "  for batch, (images, labels) in enumerate(train_loader):\n",
        "    # Getting all the images in the batch\n",
        "    images = images.reshape(-1, 28*28).to(device) # We can either do this, or we can use the flatten layer in the model.\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(images)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad() # setting the gradients to zero\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step()  # optimizing our parameters\n",
        "\n",
        "    if (batch+1)%100 == 0:\n",
        "      print(f'Epoch: {epoch+1}/{EPOCHS} | Batch: {batch+1}/{total_batches} | Train Loss: {loss.item():.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iezfwTNwB7oa",
        "outputId": "fe783eee-7ac6-4e82-e33a-19fce7b46f12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Batch: 100/1875 | Train Loss: 0.13666\n",
            "Epoch: 1/10 | Batch: 200/1875 | Train Loss: 0.12523\n",
            "Epoch: 1/10 | Batch: 300/1875 | Train Loss: 0.39583\n",
            "Epoch: 1/10 | Batch: 400/1875 | Train Loss: 0.23039\n",
            "Epoch: 1/10 | Batch: 500/1875 | Train Loss: 0.13093\n",
            "Epoch: 1/10 | Batch: 600/1875 | Train Loss: 0.11467\n",
            "Epoch: 1/10 | Batch: 700/1875 | Train Loss: 0.34180\n",
            "Epoch: 1/10 | Batch: 800/1875 | Train Loss: 0.05690\n",
            "Epoch: 1/10 | Batch: 900/1875 | Train Loss: 0.30829\n",
            "Epoch: 1/10 | Batch: 1000/1875 | Train Loss: 0.03326\n",
            "Epoch: 1/10 | Batch: 1100/1875 | Train Loss: 0.11187\n",
            "Epoch: 1/10 | Batch: 1200/1875 | Train Loss: 0.17992\n",
            "Epoch: 1/10 | Batch: 1300/1875 | Train Loss: 0.00757\n",
            "Epoch: 1/10 | Batch: 1400/1875 | Train Loss: 0.44022\n",
            "Epoch: 1/10 | Batch: 1500/1875 | Train Loss: 0.10371\n",
            "Epoch: 1/10 | Batch: 1600/1875 | Train Loss: 0.01951\n",
            "Epoch: 1/10 | Batch: 1700/1875 | Train Loss: 0.23654\n",
            "Epoch: 1/10 | Batch: 1800/1875 | Train Loss: 0.08888\n",
            "Epoch: 2/10 | Batch: 100/1875 | Train Loss: 0.02164\n",
            "Epoch: 2/10 | Batch: 200/1875 | Train Loss: 0.11649\n",
            "Epoch: 2/10 | Batch: 300/1875 | Train Loss: 0.02917\n",
            "Epoch: 2/10 | Batch: 400/1875 | Train Loss: 0.03897\n",
            "Epoch: 2/10 | Batch: 500/1875 | Train Loss: 0.00584\n",
            "Epoch: 2/10 | Batch: 600/1875 | Train Loss: 0.03092\n",
            "Epoch: 2/10 | Batch: 700/1875 | Train Loss: 0.08157\n",
            "Epoch: 2/10 | Batch: 800/1875 | Train Loss: 0.02028\n",
            "Epoch: 2/10 | Batch: 900/1875 | Train Loss: 0.11859\n",
            "Epoch: 2/10 | Batch: 1000/1875 | Train Loss: 0.08454\n",
            "Epoch: 2/10 | Batch: 1100/1875 | Train Loss: 0.24010\n",
            "Epoch: 2/10 | Batch: 1200/1875 | Train Loss: 0.00226\n",
            "Epoch: 2/10 | Batch: 1300/1875 | Train Loss: 0.03389\n",
            "Epoch: 2/10 | Batch: 1400/1875 | Train Loss: 0.04157\n",
            "Epoch: 2/10 | Batch: 1500/1875 | Train Loss: 0.01025\n",
            "Epoch: 2/10 | Batch: 1600/1875 | Train Loss: 0.00531\n",
            "Epoch: 2/10 | Batch: 1700/1875 | Train Loss: 0.01970\n",
            "Epoch: 2/10 | Batch: 1800/1875 | Train Loss: 0.01482\n",
            "Epoch: 3/10 | Batch: 100/1875 | Train Loss: 0.16115\n",
            "Epoch: 3/10 | Batch: 200/1875 | Train Loss: 0.01317\n",
            "Epoch: 3/10 | Batch: 300/1875 | Train Loss: 0.05952\n",
            "Epoch: 3/10 | Batch: 400/1875 | Train Loss: 0.00169\n",
            "Epoch: 3/10 | Batch: 500/1875 | Train Loss: 0.09210\n",
            "Epoch: 3/10 | Batch: 600/1875 | Train Loss: 0.03299\n",
            "Epoch: 3/10 | Batch: 700/1875 | Train Loss: 0.02456\n",
            "Epoch: 3/10 | Batch: 800/1875 | Train Loss: 0.06154\n",
            "Epoch: 3/10 | Batch: 900/1875 | Train Loss: 0.38343\n",
            "Epoch: 3/10 | Batch: 1000/1875 | Train Loss: 0.08327\n",
            "Epoch: 3/10 | Batch: 1100/1875 | Train Loss: 0.05635\n",
            "Epoch: 3/10 | Batch: 1200/1875 | Train Loss: 0.00118\n",
            "Epoch: 3/10 | Batch: 1300/1875 | Train Loss: 0.03305\n",
            "Epoch: 3/10 | Batch: 1400/1875 | Train Loss: 0.22908\n",
            "Epoch: 3/10 | Batch: 1500/1875 | Train Loss: 0.09831\n",
            "Epoch: 3/10 | Batch: 1600/1875 | Train Loss: 0.02930\n",
            "Epoch: 3/10 | Batch: 1700/1875 | Train Loss: 0.04529\n",
            "Epoch: 3/10 | Batch: 1800/1875 | Train Loss: 0.00085\n",
            "Epoch: 4/10 | Batch: 100/1875 | Train Loss: 0.00018\n",
            "Epoch: 4/10 | Batch: 200/1875 | Train Loss: 0.00437\n",
            "Epoch: 4/10 | Batch: 300/1875 | Train Loss: 0.01678\n",
            "Epoch: 4/10 | Batch: 400/1875 | Train Loss: 0.19773\n",
            "Epoch: 4/10 | Batch: 500/1875 | Train Loss: 0.00749\n",
            "Epoch: 4/10 | Batch: 600/1875 | Train Loss: 0.01700\n",
            "Epoch: 4/10 | Batch: 700/1875 | Train Loss: 0.00305\n",
            "Epoch: 4/10 | Batch: 800/1875 | Train Loss: 0.01759\n",
            "Epoch: 4/10 | Batch: 900/1875 | Train Loss: 0.00185\n",
            "Epoch: 4/10 | Batch: 1000/1875 | Train Loss: 0.01002\n",
            "Epoch: 4/10 | Batch: 1100/1875 | Train Loss: 0.02689\n",
            "Epoch: 4/10 | Batch: 1200/1875 | Train Loss: 0.03072\n",
            "Epoch: 4/10 | Batch: 1300/1875 | Train Loss: 0.06828\n",
            "Epoch: 4/10 | Batch: 1400/1875 | Train Loss: 0.00146\n",
            "Epoch: 4/10 | Batch: 1500/1875 | Train Loss: 0.00211\n",
            "Epoch: 4/10 | Batch: 1600/1875 | Train Loss: 0.03963\n",
            "Epoch: 4/10 | Batch: 1700/1875 | Train Loss: 0.03254\n",
            "Epoch: 4/10 | Batch: 1800/1875 | Train Loss: 0.00034\n",
            "Epoch: 5/10 | Batch: 100/1875 | Train Loss: 0.00421\n",
            "Epoch: 5/10 | Batch: 200/1875 | Train Loss: 0.00773\n",
            "Epoch: 5/10 | Batch: 300/1875 | Train Loss: 0.00507\n",
            "Epoch: 5/10 | Batch: 400/1875 | Train Loss: 0.09528\n",
            "Epoch: 5/10 | Batch: 500/1875 | Train Loss: 0.00027\n",
            "Epoch: 5/10 | Batch: 600/1875 | Train Loss: 0.17550\n",
            "Epoch: 5/10 | Batch: 700/1875 | Train Loss: 0.00068\n",
            "Epoch: 5/10 | Batch: 800/1875 | Train Loss: 0.01093\n",
            "Epoch: 5/10 | Batch: 900/1875 | Train Loss: 0.05971\n",
            "Epoch: 5/10 | Batch: 1000/1875 | Train Loss: 0.13155\n",
            "Epoch: 5/10 | Batch: 1100/1875 | Train Loss: 0.04275\n",
            "Epoch: 5/10 | Batch: 1200/1875 | Train Loss: 0.00049\n",
            "Epoch: 5/10 | Batch: 1300/1875 | Train Loss: 0.12479\n",
            "Epoch: 5/10 | Batch: 1400/1875 | Train Loss: 0.21395\n",
            "Epoch: 5/10 | Batch: 1500/1875 | Train Loss: 0.04439\n",
            "Epoch: 5/10 | Batch: 1600/1875 | Train Loss: 0.03838\n",
            "Epoch: 5/10 | Batch: 1700/1875 | Train Loss: 0.01408\n",
            "Epoch: 5/10 | Batch: 1800/1875 | Train Loss: 0.02971\n",
            "Epoch: 6/10 | Batch: 100/1875 | Train Loss: 0.00002\n",
            "Epoch: 6/10 | Batch: 200/1875 | Train Loss: 0.08469\n",
            "Epoch: 6/10 | Batch: 300/1875 | Train Loss: 0.00129\n",
            "Epoch: 6/10 | Batch: 400/1875 | Train Loss: 0.00022\n",
            "Epoch: 6/10 | Batch: 500/1875 | Train Loss: 0.00048\n",
            "Epoch: 6/10 | Batch: 600/1875 | Train Loss: 0.00333\n",
            "Epoch: 6/10 | Batch: 700/1875 | Train Loss: 0.02598\n",
            "Epoch: 6/10 | Batch: 800/1875 | Train Loss: 0.00371\n",
            "Epoch: 6/10 | Batch: 900/1875 | Train Loss: 0.00059\n",
            "Epoch: 6/10 | Batch: 1000/1875 | Train Loss: 0.00182\n",
            "Epoch: 6/10 | Batch: 1100/1875 | Train Loss: 0.01691\n",
            "Epoch: 6/10 | Batch: 1200/1875 | Train Loss: 0.27636\n",
            "Epoch: 6/10 | Batch: 1300/1875 | Train Loss: 0.00179\n",
            "Epoch: 6/10 | Batch: 1400/1875 | Train Loss: 0.05477\n",
            "Epoch: 6/10 | Batch: 1500/1875 | Train Loss: 0.10399\n",
            "Epoch: 6/10 | Batch: 1600/1875 | Train Loss: 0.01192\n",
            "Epoch: 6/10 | Batch: 1700/1875 | Train Loss: 0.00023\n",
            "Epoch: 6/10 | Batch: 1800/1875 | Train Loss: 0.02641\n",
            "Epoch: 7/10 | Batch: 100/1875 | Train Loss: 0.00273\n",
            "Epoch: 7/10 | Batch: 200/1875 | Train Loss: 0.01122\n",
            "Epoch: 7/10 | Batch: 300/1875 | Train Loss: 0.00060\n",
            "Epoch: 7/10 | Batch: 400/1875 | Train Loss: 0.00250\n",
            "Epoch: 7/10 | Batch: 500/1875 | Train Loss: 0.00025\n",
            "Epoch: 7/10 | Batch: 600/1875 | Train Loss: 0.02656\n",
            "Epoch: 7/10 | Batch: 700/1875 | Train Loss: 0.00004\n",
            "Epoch: 7/10 | Batch: 800/1875 | Train Loss: 0.01259\n",
            "Epoch: 7/10 | Batch: 900/1875 | Train Loss: 0.02712\n",
            "Epoch: 7/10 | Batch: 1000/1875 | Train Loss: 0.12306\n",
            "Epoch: 7/10 | Batch: 1100/1875 | Train Loss: 0.00018\n",
            "Epoch: 7/10 | Batch: 1200/1875 | Train Loss: 0.00061\n",
            "Epoch: 7/10 | Batch: 1300/1875 | Train Loss: 0.00060\n",
            "Epoch: 7/10 | Batch: 1400/1875 | Train Loss: 0.03321\n",
            "Epoch: 7/10 | Batch: 1500/1875 | Train Loss: 0.00061\n",
            "Epoch: 7/10 | Batch: 1600/1875 | Train Loss: 0.04242\n",
            "Epoch: 7/10 | Batch: 1700/1875 | Train Loss: 0.02749\n",
            "Epoch: 7/10 | Batch: 1800/1875 | Train Loss: 0.03909\n",
            "Epoch: 8/10 | Batch: 100/1875 | Train Loss: 0.00013\n",
            "Epoch: 8/10 | Batch: 200/1875 | Train Loss: 0.00241\n",
            "Epoch: 8/10 | Batch: 300/1875 | Train Loss: 0.02471\n",
            "Epoch: 8/10 | Batch: 400/1875 | Train Loss: 0.05101\n",
            "Epoch: 8/10 | Batch: 500/1875 | Train Loss: 0.00002\n",
            "Epoch: 8/10 | Batch: 600/1875 | Train Loss: 0.00083\n",
            "Epoch: 8/10 | Batch: 700/1875 | Train Loss: 0.00018\n",
            "Epoch: 8/10 | Batch: 800/1875 | Train Loss: 0.18464\n",
            "Epoch: 8/10 | Batch: 900/1875 | Train Loss: 0.00415\n",
            "Epoch: 8/10 | Batch: 1000/1875 | Train Loss: 0.00034\n",
            "Epoch: 8/10 | Batch: 1100/1875 | Train Loss: 0.00876\n",
            "Epoch: 8/10 | Batch: 1200/1875 | Train Loss: 0.00181\n",
            "Epoch: 8/10 | Batch: 1300/1875 | Train Loss: 0.00269\n",
            "Epoch: 8/10 | Batch: 1400/1875 | Train Loss: 0.00005\n",
            "Epoch: 8/10 | Batch: 1500/1875 | Train Loss: 0.00729\n",
            "Epoch: 8/10 | Batch: 1600/1875 | Train Loss: 0.00427\n",
            "Epoch: 8/10 | Batch: 1700/1875 | Train Loss: 0.00001\n",
            "Epoch: 8/10 | Batch: 1800/1875 | Train Loss: 0.00042\n",
            "Epoch: 9/10 | Batch: 100/1875 | Train Loss: 0.00174\n",
            "Epoch: 9/10 | Batch: 200/1875 | Train Loss: 0.00005\n",
            "Epoch: 9/10 | Batch: 300/1875 | Train Loss: 0.02176\n",
            "Epoch: 9/10 | Batch: 400/1875 | Train Loss: 0.00009\n",
            "Epoch: 9/10 | Batch: 500/1875 | Train Loss: 0.00006\n",
            "Epoch: 9/10 | Batch: 600/1875 | Train Loss: 0.00015\n",
            "Epoch: 9/10 | Batch: 700/1875 | Train Loss: 0.00001\n",
            "Epoch: 9/10 | Batch: 800/1875 | Train Loss: 0.00030\n",
            "Epoch: 9/10 | Batch: 900/1875 | Train Loss: 0.00058\n",
            "Epoch: 9/10 | Batch: 1000/1875 | Train Loss: 0.00786\n",
            "Epoch: 9/10 | Batch: 1100/1875 | Train Loss: 0.00015\n",
            "Epoch: 9/10 | Batch: 1200/1875 | Train Loss: 0.00035\n",
            "Epoch: 9/10 | Batch: 1300/1875 | Train Loss: 0.00038\n",
            "Epoch: 9/10 | Batch: 1400/1875 | Train Loss: 0.00185\n",
            "Epoch: 9/10 | Batch: 1500/1875 | Train Loss: 0.00029\n",
            "Epoch: 9/10 | Batch: 1600/1875 | Train Loss: 0.00822\n",
            "Epoch: 9/10 | Batch: 1700/1875 | Train Loss: 0.00113\n",
            "Epoch: 9/10 | Batch: 1800/1875 | Train Loss: 0.00014\n",
            "Epoch: 10/10 | Batch: 100/1875 | Train Loss: 0.00424\n",
            "Epoch: 10/10 | Batch: 200/1875 | Train Loss: 0.03301\n",
            "Epoch: 10/10 | Batch: 300/1875 | Train Loss: 0.00009\n",
            "Epoch: 10/10 | Batch: 400/1875 | Train Loss: 0.00076\n",
            "Epoch: 10/10 | Batch: 500/1875 | Train Loss: 0.06003\n",
            "Epoch: 10/10 | Batch: 600/1875 | Train Loss: 0.00239\n",
            "Epoch: 10/10 | Batch: 700/1875 | Train Loss: 0.00010\n",
            "Epoch: 10/10 | Batch: 800/1875 | Train Loss: 0.00004\n",
            "Epoch: 10/10 | Batch: 900/1875 | Train Loss: 0.00359\n",
            "Epoch: 10/10 | Batch: 1000/1875 | Train Loss: 0.05959\n",
            "Epoch: 10/10 | Batch: 1100/1875 | Train Loss: 0.08120\n",
            "Epoch: 10/10 | Batch: 1200/1875 | Train Loss: 0.00002\n",
            "Epoch: 10/10 | Batch: 1300/1875 | Train Loss: 0.02614\n",
            "Epoch: 10/10 | Batch: 1400/1875 | Train Loss: 0.00653\n",
            "Epoch: 10/10 | Batch: 1500/1875 | Train Loss: 0.02713\n",
            "Epoch: 10/10 | Batch: 1600/1875 | Train Loss: 0.00000\n",
            "Epoch: 10/10 | Batch: 1700/1875 | Train Loss: 0.00007\n",
            "Epoch: 10/10 | Batch: 1800/1875 | Train Loss: 0.38551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing Loop\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    # returns value, index. We only need index i.e the class label\n",
        "    _, predictions = torch.max(outputs, dim=1)  # returns the index having the highest value.\n",
        "    n_samples += labels.shape[0]  # Gives us number of samples in current batch (should be 32)\n",
        "    n_correct += torch.eq(predictions, labels).sum().item() # adding 1 for every correct prediction.\n",
        "\n",
        "  acc = 100 * n_correct / n_samples\n",
        "  print(f'accuracy = {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HZgTZJNDGWU",
        "outputId": "a855bac6-9c28-41db-b76d-e8295fc46c80"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 98.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXivBB8UEZ_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}